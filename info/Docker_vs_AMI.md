

Below I try and summarize what I think are the promising features of Docker relative to simply using something like EC2 Amazon Machine Images (AMIs).  I don't think this means it is *the solution*, but I do think it illustrates some very useful steps forward to important issues in reproducibility and virtualization.  Remember, Docker is still a very young and rapidly evolving platform.

1) Remix.  Titus has an excellent post, "Virtual Machines Considered Harmful for reproducibility" [1] , essentially pointing out that "you can't install an image for every pipeline you want...".  In contrast, Docker containers are designed to work exactly like that -- reusable building blocks you can link together with very little overhead in disk space or computation.  This more than anything else sets Docker apart from the standard VM approach.

2) Provisioning scripts. Docker images are not 'black boxes'. A "Dockerfile" is a simple make-like script which installs all the software necessary to re-create ("provision") the image.  This has many advantages: (a) The script is much smaller and more portable than the image. (b) the script can be version managed (c) the script gives a human readable (instead of binary) description of what software is installed and how. This also avoids pitfalls of traditional documentation of dependencies that may be too vague or out-of-sync.  (d) Other users can build on, modify, or extend the script for their own needs.  All of this is what we call the he "DevOpts" approach to provisioning, and can be done with AMIs or other virtual machines using tools like Ansible, Chef, or Puppet coupled with things like Packer or Vagrant (or clever use of make and shell scripts).

For a much better overview of this "DevOpts" approach in the reproducible research context and a gentle introduction to these tools, I highly recommend taking a look at Clark et al [2].

3) You can run the docker container *locally*.  I think this is huge.  In my experience, most researchers do their primary development locally.  By running RStudio-server on your laptop, it isn't necessary for me to spin up an EC2 instance (with all the knowledge & potential cost that requires).  By sharing directories between Docker and the host OS, a user can still use everything they already know -- their favorite editor, moving files around with the native OS finder/browser, using all local configurations, etc, while still having the code execution occur in the container where the software is precisely specified and portable.  Whenever you need more power, you can then deploy the image on Amazon, DigitalOcean, a bigger desktop, your university HPC cluster, your favorite CI platform, or wherever else you want your code to run.  (On Mac & Windows, this uses something called boot2docker, and was not very seamless early on.  It has gotten much better and continues to improve.)

4) Versioned images.  In addition to version managing the Dockerfile, the images themselves are versioned using a git-like hash system (check out: docker commit, docker push/pull, docker history, docker diff, etc).  They have metadata specifying the date, author, parent image, etc.  We can roll back an image through the layers of history of its construction, then build off an earlier layer.  This also allows docker to do all sorts of clever things, like avoiding downloading redundant software layers from the docker hub.  (If you pull a bunch of images that all build on ubuntu, you don't get n copies of ubuntu you have to download and store).  Oh yeah, and hosting your images on Docker hub is free (no need to pay for an S3 bucket... for now?) and supports automated builds based on your dockerfiles, which acts as a kind of CI for your environment.  Versioning and diff\ing images is a rather nice reproducibility feature.

[1]: http://ivory.idyll.org/blog/vms-considered-harmful.html
[2]: https://berkeley.box.com/s/w424gdjot3tgksidyyfl


